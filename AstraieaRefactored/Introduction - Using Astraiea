=================ASTRAIEA 2015==========================
By Geoffrey Neumann, Jerry Swan, Mark Harman and John Clark

Astraiea is a tool kit for the fair comparison of experimental results.
It is designed to comply with the seminal paper on statistical methods for software engineering by 
Andrea Arcuri and Lional Briand: 
"A Hitchhiker’s guide to statistical tests for assessing randomized algorithms in software engineering" (Arcuri2012).

Astraiea takes as input two sets of results or a collection of programs for producing results and carries out a fair comparison.

It will print a LaTeX file giving the results of these comparisons which can then be added to research papers.
Below is explained how Astraiea may be used to carry out this comparison and how it may be configured to 
carry out various different tests depending on the nature of the data.

========================================================

Astraiea may be accessed through one of the two layers of which it is comprised:
1) Running through Layer 1: Comparing two sets of results already obtained.
2) Running through Layer 2: Providing Astraiea with two or more result generators which will then be invoked. 
The output of these will be input in pairs into layer 1 which will then compare them.

==============Layer 1==================================

1.1 - DEFAULT: 

By default, according to the instructions in (Arcuri2012), two datasets will be compared using:
1) The Wilcoxon/Mann-Whitney U test to test statistical significance.
2) The Vargha Delaney test to test effect size.
3) Confidence intervals are obtained on the effect size through bootstrapping.
The intervals are the same size as the significance threshold 
i.e. with a threshold of 0.05 confidence intervals at 45% and 55% are shown.

An example of calling layer1 with this default set up:
(For more information on this or any of the other examples for Layer1, see the javadoc for the "Layer1" class)

Random ran = new Random();
Layer1.compare( dataA, dataB, double significanceThreshold, boolean brunnerMunzel, Random ran );

"dataA" and "dataB" may be either arrays of lists of doubles.

1.2 OTHER COMPARISONS

In addition the following comparisons may be performed using Layer1:

The Brunner Munzel test-
Although the Wilcoxon P Value test is the default as it is recommended in Lionel and Briand's paper, 
this test has the disadvantage that it assumes that the data is not heteroscedastic.
The Brunner Munzel test does not make this assumption and so it is more robust.

Comparing an array with a single data point-
A set of results can be compared with a single result i.e. when comparing the results of a stochastic algorithm with a deterministic one.
For this the mann-whitney one sample test is used.
TODO effect size is currently not implemented as I don't know which effect size test should be used.

Comparing paired data-
This is for situations in which each sample in dataA is paired to a sample in dataB.
This may be because the comparison is between two treatments carried out on one entity.
For this the Wilcoxon Signed rank test of significance followed by the a paired version of the Vargha Delaney effect size test.


Censored data-
This is for dichotomous tests where results can be cateogorized either as a pass or a fail at the point where the test finished.
The fisher p value test followed by the odds ratio effect size test is used.

Modified Vargha Delaney-
This allows the application of a modified version of the Vargha Delaney effect size test which allows it to be customised to the problem (explained in ....). 
An overwritten Layer.compare method is called to utilise this which is the same as the standard method except for the addition of a parameter of type VDmod:

==============Layer 2==================================

2.1 - DEFAULT: 

By default 2 or more generators (e.g. algorithms) are each invoked for a fixed number of times and the results compared through layer 1.
The method call is as follows:
Layer2.run(gens, double significanceThreshold, boolean brunnerMunzel, boolean paired, IncrementingStrategy incr, Random random)
"gens" may be either of class "DatapointGeneratorSet" or "TimeseriesGeneratorSet".
Contained within "gens" is the set of generators to be compared, 
information as to whether every pairwise comparison will be performed or whether each generator will only be compared to a single generator
and information about any multiple test correction used (like Bon Ferroni).

2.2 OTHER COMPARISONS

Censored data strategies:
Censored data means that a result is classed as true or false based on an observation taken at whichever point the test finished.
This is, in some sense, an arbitrary judgement. 
For this reason, Arcuri and Briand's paper suggests various alternative tests to be carried out after the initial censored test.
These are made available in Astraiea by passing an object of class CensoringStrategy. See the documentation for this class for more information.
For datapoint generators no alternative strategies exist and so the method call to Layer2 is simply this:
Layer2.runCensored(DatapointGeneratorSet gens,double significanceThreshold,boolean paired,IncrementingStrategy incr,Random random)
However, for time series generators the particular choice as to which alternative strategies are used 
is encapsulated into an object of class "CensoringStrategy" passed to Layer2.

Incrementing data:
It is possible to carry out additional invocations of each generator if the initial n invocations proved to be insufficient to demonstrate significance.
This process must be carried out in a fair way and the data must be adjusted accordingly. Various alternatives methods for doing this can be plugged into Astraiea.
How Incrementing should be performed is encapsulated into an object of class "IncrementingStrategy" passed to Layer2. 

Multiple artefacts:
This is when generators are invoked on multiple instances of a problem.
This is dealt with in Astraiea in the manner recommended by Arcuri and Briand. 
For each set of runs on one artefact the median is taken, thus producing one value for each artefact. 
It is these values which are then used in statistical testing in the normal manner.
