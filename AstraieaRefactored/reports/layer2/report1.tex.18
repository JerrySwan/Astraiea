\documentclass[]{article}
\usepackage[usenames,dvipsnames]{color}
\usepackage{longtable}
\begin{document}

\section{Experimental Results (Wed Nov 25 23:46:20 GMT 2015)}
Experiments were performed using the \textsc{Astraiea}\footnote{This file has been automatically generated by Astraiea} statistical testing framework \cite{Neumann:2014:EET:2598394.2609850},
which performs tests in accordance with the guidelines of ``A Hitchhikers guide to statistical testing''
by Briand and Arcuri~\cite{Arcuri2012}.
Unless a different reference is given, the tests are performed as described in this paper.

Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The Wilcoxon/ Mann Whitney U Significance Test was used.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 0.02558535265900963, threshold of 0.05)}
\item{the effect size is 0.3322222222222222, confidence intervals (obtained by bootstrapping)= 0.18894444444444444 - 0.4922222222222222}
\item{null is lower than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The Brunner Munzel P Value Significance Test was used. Brunner Munzel is used in place of the  Wilcoxon/ Mann Whitney U test recommended in Arcuri's paper~\cite{Arcuri2012} because the Brunner Munzel test is tolerant of heteroscedastic data whereas the Wilcoxon/ Mann Whitney U test is not~\cite{Brunner2000}.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 2.0199143460808422E-4, threshold of 0.05)}
\item{the effect size is 0.24888888888888888, confidence intervals (obtained by bootstrapping)= 0.13555555555555557 - 0.37444444444444447}
\item{null is lower than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The data is paired. A paired version of the Wilcoxon/Mann-Whitney U test for significance will be used.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 2.6098434503518675E-6, threshold of 0.05)}
\item{the effect size is 2.5}
\item{null is greater than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The data is dichotomous and paired and so the Fisher test of statistical significance was used followed by the Odds Ratio test of effect size.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. The initial censoring strategy was used (using a dichotomous test on the results at the end of the time series). 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 2.440523822271272E-7, threshold of 0.05)}
\item{the effect size is 0.04061895551257254}
\item{null is lower than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The data is dichotomous and paired and so the McNemar test of statistical significance~\cite{Gibbons2011} was used followed by the Matched Odds Ratio test of effect size.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. The initial censoring strategy was used (using a dichotomous test on the results at the end of the time series). 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 3.005976075328709E-4, threshold of 0.05)}
\item{the effect size is 0.03225806451612903}
\item{null is lower than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The Wilcoxon/ Mann Whitney U Significance Test was used.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{data is not significant (p value of 0.4687975862460966, threshold of 0.05)}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The Wilcoxon/ Mann Whitney U Significance Test was used.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 6.505728436412284E-6, threshold of 0.05)}
\item{the effect size is 0.16111111111111112, confidence intervals (obtained by bootstrapping)= 0.06777777777777778 - 0.27}
\item{null is lower than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The data is dichotomous and paired and so the Fisher test of statistical significance was used followed by the Odds Ratio test of effect size.These tests compare boolean values, denoting a pass or a fail, from the final point in the result time series. 

This data is censored, that is to say the point at which results are obtained is the arbitrary point at which the test stopped running. This may not accurately reflect the true difference between the two datasets. For this reason, additional statistical tests were carried out if this initial test did not show significance. Additional tests where carried out in this order:
\begin{itemize}
\item{0: Tests were run using an artificial censoring point at point -1 in the time series.}
\item{2: Tests were run using a non dichotomous P Value test in which the length of time taken to reach a successful result is compared("Time to Pass" test).}
\end{itemize}Note that using these strategies involves multiple P value calculations on the same data. This should be taken into account when interpreting these results.
}
\item{The points below only apply to the non dichotomous time based test:}
\item{The Wilcoxon/ Mann Whitney U Significance Test was used.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. The time to pass strategy for censored data was used (using a non dichotomous p value test on the length of time that each successful run took to reach a passing value). 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 0.035027035062719536, threshold of 0.05)}
\item{the effect size is 0.6583333333333334, confidence intervals (obtained by bootstrapping)= 0.5195277777777778 - 0.7782777777777777}
\item{null is greater than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.
\textcolor{Red}{WARNING: The Brunner Munzel test has been requested. This is not relevant for dichotomous tests and will only be used if a non dichotomous test is carried out.}
\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The data is dichotomous and paired and so the Fisher test of statistical significance was used followed by the Odds Ratio test of effect size.These tests compare boolean values, denoting a pass or a fail, from the final point in the result time series. 

This data is censored, that is to say the point at which results are obtained is the arbitrary point at which the test stopped running. This may not accurately reflect the true difference between the two datasets. For this reason, additional statistical tests were carried out if this initial test did not show significance. Additional tests where carried out in this order:
\begin{itemize}
\item{0: Tests were run using an artificial censoring point at point -1 in the time series.}
\item{2: Tests were run using a non dichotomous P Value test in which the length of time taken to reach a successful result is compared("Time to Pass" test).}
\end{itemize}Note that using these strategies involves multiple P value calculations on the same data. This should be taken into account when interpreting these results.
}
\item{The points below only apply to the non dichotomous time based test:}
\item{The Brunner Munzel P Value Significance Test was used. Brunner Munzel is used in place of the  Wilcoxon/ Mann Whitney U test recommended in Arcuri's paper~\cite{Arcuri2012} because the Brunner Munzel test is tolerant of heteroscedastic data whereas the Wilcoxon/ Mann Whitney U test is not~\cite{Brunner2000}.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. The time to pass strategy for censored data was used (using a non dichotomous p value test on the length of time that each successful run took to reach a passing value). 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 0.027346879788939793, threshold of 0.05)}
\item{the effect size is 0.6583333333333334, confidence intervals (obtained by bootstrapping)= 0.5195277777777778 - 0.7782777777777777}
\item{null is greater than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{Using the process of incrementing the number of runs until the difference is statistically significant. Initially 30 experiments were run. If sigificance is not obtained from these experiments then additional experiments are run until either significance is obtained or a maximum of 100 experiments have been run. When this technique is used effect size testing is especially important as a number of samples sufficient to show a statistically significant difference is likely to be reached even if the difference is too small to be useful. Note that this strategy involves multiple P value calculations as $n$ increases. These have not been adjusted and issues related to multiple testing should be taken into account when interpreting these results.
}
\item{The Wilcoxon/ Mann Whitney U Significance Test was used.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 0.02558535265900963, threshold of 0.05)}
\item{the effect size is 0.3322222222222222, confidence intervals (obtained by bootstrapping)= 0.18788888888888888 - 0.4922222222222222}
\item{null is lower than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{Using the process of incrementing the number of runs until the difference is statistically significant. Initially 30 experiments were run. If sigificance is not obtained from these experiments then additional experiments are run until either significance is obtained or a maximum of 100 experiments have been run. When this technique is used effect size testing is especially important as a number of samples sufficient to show a statistically significant difference is likely to be reached even if the difference is too small to be useful. Note that this strategy involves multiple P value calculations as $n$ increases. These have not been adjusted and issues related to multiple testing should be taken into account when interpreting these results.
}
\item{The Wilcoxon/ Mann Whitney U Significance Test was used.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 8.339303300103351E-4, threshold of 0.05)}
\item{the effect size is 0.24888888888888888, confidence intervals (obtained by bootstrapping)= 0.13555555555555557 - 0.37655555555555537}
\item{null is lower than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The data is dichotomous and paired and so the Fisher test of statistical significance was used followed by the Odds Ratio test of effect size.These tests compare boolean values, denoting a pass or a fail, from the final point in the result time series. 

This data is censored, that is to say the point at which results are obtained is the arbitrary point at which the test stopped running. This may not accurately reflect the true difference between the two datasets. For this reason, additional statistical tests were carried out if this initial test did not show significance. Additional tests where carried out in this order:
\begin{itemize}
\item{0: Tests were run using an artificial censoring point at point -1 in the time series.}
\end{itemize}Note that using these strategies involves multiple P value calculations on the same data. This should be taken into account when interpreting these results.
}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. The initial censoring strategy was used (using a dichotomous test on the results at the end of the time series). 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 9.162440773562147E-4, threshold of 0.05)}
\item{the effect size is 0.13099730458221023}
\item{null is lower than null}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{All experiments were repeated 30 times ($n$=30)}
\item{The data is dichotomous and paired and so the Fisher test of statistical significance was used followed by the Odds Ratio test of effect size.These tests compare boolean values, denoting a pass or a fail, from the final point in the result time series. 

This data is censored, that is to say the point at which results are obtained is the arbitrary point at which the test stopped running. This may not accurately reflect the true difference between the two datasets. For this reason, additional statistical tests were carried out if this initial test did not show significance. Additional tests where carried out in this order:
\begin{itemize}
\item{0: Tests were run using an artificial censoring point at point -1 in the time series.}
\item{1: Tests were run using an artificial censoring point at point 50 in the time series.}
\end{itemize}Note that using these strategies involves multiple P value calculations on the same data. This should be taken into account when interpreting these results.
}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 30. An intermediate, artificial censoring point was used of 50.

The final results from comparing null and null are as follows:
\begin{itemize}
\item{data is not significant (p value of 0.9999999999999929, threshold of 0.05)}
\end{itemize}Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{Using the process of incrementing the number of runs until the difference is statistically significant. Initially 20 experiments were run. If sigificance is not obtained from these experiments then additional experiments are run until either significance is obtained or a maximum of 100 experiments have been run. When this technique is used effect size testing is especially important as a number of samples sufficient to show a statistically significant difference is likely to be reached even if the difference is too small to be useful. Note that this strategy involves multiple P value calculations as $n$ increases. These have not been adjusted and issues related to multiple testing should be taken into account when interpreting these results.
}
\item{The Wilcoxon/ Mann Whitney U Significance Test was used.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 100. 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{data is not significant (p value of 0.49888051907411923, threshold of 0.05)}
\end{itemize}