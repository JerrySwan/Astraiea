\documentclass[]{article}
\usepackage[usenames,dvipsnames]{color}
\usepackage{longtable}
\begin{document}

\section{Experimental Results (Thu Nov 26 00:38:38 GMT 2015)}
Experiments were performed using the \textsc{Astraiea}\footnote{This file has been automatically generated by Astraiea} statistical testing framework \cite{Neumann:2014:EET:2598394.2609850},
which performs tests in accordance with the guidelines of ``A Hitchhikers guide to statistical testing''
by Briand and Arcuri~\cite{Arcuri2012}.
Unless a different reference is given, the tests are performed as described in this paper.

Statistical testing was carried out as follows: 
This data was obtained from runs on multiple artefacts. Each artefact is treated as a single run for the purpose of statistical comparison and so, the experimental description below,  the number of runs refers to the number of artefacts. For each artefact 1 repeated tests were carried out and the median of these results was used as the result for that artefact for the purposes of subsequent statistical testing.\begin{itemize}
\item{Using the process of incrementing the number of runs until the difference is statistically significant. Initially 20 experiments were run. If sigificance is not obtained from these experiments then additional experiments are run until either significance is obtained or a maximum of 100 experiments have been run. When this technique is used effect size testing is especially important as a number of samples sufficient to show a statistically significant difference is likely to be reached even if the difference is too small to be useful. Note that this strategy involves multiple P value calculations as $n$ increases. These have not been adjusted and issues related to multiple testing should be taken into account when interpreting these results.
}
\item{The Wilcoxon/ Mann Whitney U Significance Test was used.}
\item{The Vargha Delaney Effect Size Test was used. Effect size testing is essential in addition to significance testing as it demonstrates the magnitude of the difference between two samples. With a large enough number of experiments (large enough $n$), the results of two different generating techniques are likely to be different to a statistically significant extent. Effect size testing is needed to show that this difference is useful.}
\end{itemize}A complete list of p value tests carried out is shown in table~\ref{p value tests}, with $n$ corresponding to the number of samples in each data set.
\begin{center}
\begin{longtable}{|l|l|l|}
\caption[P Value Tests]{P Value Tests} \label{p value tests} \\ 
\hline \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\\ \hline 
\endfirsthead 
\multicolumn{3}{c}{{\bfseries \tablename\ \thetable{} -- continued from previous page}} \\ 
 \hline 
 \multicolumn{1}{|c|}{\textbf{n}} &  \multicolumn{1}{|c|}{\textbf{Notes}} &  \multicolumn{1}{|c|}{\textbf{P-Value}}
\endhead 
\hline \multicolumn{3}{|r|}{{Continued on next page}} \\ \hline 
\endfoot 
\hline 
\endlastfoot 

\hline
\end{longtable}
\end{center}

The final test, and the test on which effect size was calculated, was carried out using an $n$ of 100. 

The final results from comparing null and null are as follows:
\begin{itemize}
\item{difference is significant (p value of 0.006174550948616964, threshold of 0.05)}
\item{the effect size is 0.3879, confidence intervals (obtained by bootstrapping)= 0.30995 - 0.468695}
\item{null is lower than null}
\end{itemize}